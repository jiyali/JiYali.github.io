<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1"><title>论文笔记-From Captions to Visual Concepts and Back</title><link rel="shortcut icon" href="/images/avatar.png"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.2.0/styles/github.min.css"><script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script></head><body><nav class="main-nav"><a href="/">Home</a><a href="/archives">Archives</a></nav><div class="profile"><section id="wrapper"><header id="header"><img class="2x" id="avatar" src="/images/avatar.png"><h1>Coding With Yali</h1><h2></h2></header></section></div><section class="post" id="wrapper"><article><header><h1>论文笔记-From Captions to Visual Concepts and Back</h1><h2 class="headline">Feb 25, 2019 2:24·7.6k words
·26 minutes read<span class="tags"></span></h2></header><section id="post-body"><p>摘要<br>本文展示了一种自动生成图像描述的新方法：直接地从图像标题数据集中学习的视觉检测器和语言模型。我们使用的是多示例学习去训练在标题中常见词的视觉检测器，这些常见词包括很多不同的词性包括名词、动词和形容词。单词检测器的输出用作最大熵语言模型的条件输入。这个语言模型从一组超过400,000图像描述中学习，来捕捉单词用法的统计量。我们通过使用句子级的特征和多模态相似性模型来对标题候选者的重新排序，从而获得全局语义。<br>1.前言<br>什么时候说明机器理解了图片呢？一个定义是：当它可以生成一个新的标题来概述图片中突出的内容比如图片中出现了什么目标、他们的特征、或者目标之间的关系时，就说机器理解了图片。决定主要内容的不仅需要知道图片内容，而且还需要通过常识推断出场景的哪个方面可能有意思或者很新颖。<br>本文描述了一个从样本中生成图像标题的新方法。我们从图片以及相应的图像描述数据集中训练我们的标题生成器。以前生成图像标题的方法依赖于目标、特征和从单独的手动标注的训练集中学习到的关系探测器。<br>在训练中直接使用标注有两个明显的优势。首先，标题只是包含内在的显著的信息。比如说一个从包含“小狗”这个单词的很多有标题的图片中训练出来的小狗探测器可能偏向于探测突出的小狗，而不是背景中的小狗。图像描述也包括各种各样的单词，包括名词、动词还有形容词。因此，我们需要学习与各种概念相关的探测器。当有些概念，比如说“骑”或者“漂亮”，在理论上可能非常难学习。这些术语可能与特定的视觉形态高度相关（如一个人骑在马上或者日落时在山上。<br>其次，在图像标题上训练语言模型可以获取关于场景中的常识性知识。一个语言模型可以学习到一个人更可能坐在椅子上而不是站在椅子上。这个信息消除了噪声视觉检测的歧义。<br>我们的方法流程如图一所示。首先我们用弱监督学习为一组图像标注中的常用单词构造一个检测器。直接从图像标注中学习是很难的，因为系统并不能利用像对象候选边界框这种在别的数据集中找到的监督信号。很多单词，比如说“拥挤”或者“内部”，甚至没有定义良好的候选边界框。为了克服这个难点，我们使用三个想法。首先，系统发现图像领域使用目标建议生成器。下一步，我们使用预处理的ImageNet卷积神经网络特征对每个区域进行特征化。最后，我们把每个区域的特征映射到可能包含在标注中的一组词汇中去。我们用多示例学习来训练这个映射去确定每个单词信息量最大的图像区域。<br>从可能的词袋产生新的图像描述需要有效的语言模型。在本文中，我们把标注生成看做一个优化问题。这样看来，核心任务就是拿到一套单词检测的分数集，然后找出每个单词都恰好包含一次的最大可能性的句子。我们从一组训练图像描述中训练出一个最大熵语言模型。这种训练通过语言统计学获得关于这个世界的常识性知识。对单词序列进行显示搜索可以有效找出一组可能性高的句子。<br>系统的最后一阶段通过对句子特征进行加权处理重排了这组高似然句子。这些权重通过最小错误率训练学习得到。除了几个普通的句子特征，我们还介绍了一个基于深度多模态相似性模型的新的特征。深度多模态相似性模型学习两个神经网络将图像和文字片段映射到一个公共向量上，这样就能很容易的测量句子和图片之间的相似性。我们论证了，深度多模态相似性模型显著提高了选择句子的质量。<br>为了鉴定我们自动标注的质量，我们用了三个易于计算的指标，以及在“Amazon’s Mechanical Turk”上与人类受试者做了更好/更差/相等的比较。测试是在具有挑战性的”Microsoft COCO“数据集上进行的，这个数据集包含复杂的多个目标的照片。82,783训练图片每一个都有人工注释的标题。为了测定我们句子的质量，我们使用了流行的”BLEU,METEOR还有perplexity (PPLX)”指标。我们惊喜的发现，我们生成的标注在“BLEU”指标上尽然胜过了人类。当人类受试者评估时，受试者发现我们的标注在23%的情况下有着与人类同等质量甚至超过人类。我们也比较了以前在“PLSCAL”语句数据集上的准确性。我们的结果表明，直接在图像标注上同时训练视觉探测器和语言模型和使用全局语义模型来重排标题候选者都是非常有效的。<br>2.相关工作<br>有两个训练良好的自动生成图像标题的方法：检索现有的人工标注的图像标题，生成新的标题。最近基于检索的方法已经使用神经网络将文本和图像映射到一个公共向量中。其他基于检索的方法使用的是预定义的图像特征的相似性指标。Farhadi等人提出将图像和文字作为一个linguistically-motivated（暂时不懂怎么翻译）语义三元组，然后在这个语义三元组中计算相似性。在神经网络的环境下对句子和图像进行了相似的细粒度分析发现对检索的用处不大。<br>基于检索的方法经常返回一个结构良好的人工标注的标题，但是这些标题可能不能很好的描述新目标的组合或者一个新的句子。这种限制导致了大量的工作，首先需要分析图像和图像中检测到的对象，然后生成新的标题。我们把之前在生成过程中使用的语法和语义的限制的研究和当前最先进的技术进行了比较。我们研究了利用最大似然估计结合句法结构生成新句子的”Midge“系统，并将它与选择通过条件随机场预测的最可能的图像标签来填充句子模板的”Baby Talk“系统进行了定性比较。这两个先前的系统使用了相同的测试句子集，使得直接对比两个系统成为了可能。<br>最近，研究人员探索了从图像中引导语言模型的纯粹的统计方法。Kiros等人从图像派生出的文本模型中提取出图像中的偏置特征使用“log-bilinear”模型，Kiros的模型通过采样对标题进行排序和生成。Mao等人探索了图像条件递归神经网络的使用。这两种方法都没有使用明确的出事对象检测步骤。<br>类似于最后两种方法，我们的研究重点为以图像内容为引导的语言生成。不同于这些方法的是，我们通过将CNN应用到图像区域中检测图像中的单词，并将这些单词用MIL进行信息整合。我们还通过直接从标题训练来最小化句子结构的先验假设。最后，与Kiris,Mao相比，我们将生成问题当做一个最优化问题，并寻找最可能的句子。<br>3.字检测<br>我们生成标题流程的第一步是检测出一组最可能是图像描述的一部分的单词。这些单词可能属于任意一种词性，包括动词，名词和形容词。我们规定我们的词汇表V使用1000个训练标题中的常用词汇。这个词汇表包含了训练标题中出现的92%的单词。整个词汇表会在附录中展示出来。<br>3.1训练单词检测器<br>给出了词汇表，我们下一步的目标是从图像中检测单词。当我们不知道与单词对应的图像候选边界时我们不能使用标准监督学习技术。事实上，很多单词比如像“打开”或者“漂亮”的含义并不能很容易的定义它的候选边界框。一个可能的方式是使用图像分类器作为整个图像的输入。在我们展示的第6节里，这将导致性能下降因为很多单词或概念只适用于图像的子区域。相反的，我们用多实例学习的弱监督方法来训练我们的探测器。<br>属于词汇表V的每个单词ω，MIL将候选边界框的“正”“负”袋作为输入集。每个袋相当于一张图像。对于单词ω和图像i，如果单词ω在图像i的描述中时，一个包bi就会将其描述为“正”,反之为负。直观地来说，MIL通过迭代的在“正”包中寻找“正”实例来进行训练，接着用更新“正”标签的方法反复训练探测器。使用Zitnick and Doll´ar的对象建议生成器Edge Boxes 70可以生成每张照片64组候选边界框的计算效率。<br>我们使用noisy-OR版本的MIL，这个版本中包含单词ω的包bi的概率是由词袋中单个实例的概率计算的：<br>这里的（公式）包含单词ω包bi的概率取决于包中每个实例的概率：<br>这里的（公式）是图像i对应的单词ω的候选边界框j的概率。对于每个候选边界框，我们将ImageNet数据放到卷积神经网络的fc6层去训练来计算特征，这些特征已经被证明不仅仅适用于ImageNet的分类任务，而且还适用于各种视觉任务。我们对图像i中的选择框j的fc6的特征Φ(bij)使用了逻辑函数（公式）来计算p(公式）。这里的vω与单词有关系的权重，uω是偏差<br>3.2对测试图片生成单词<br>给定一个新的测试图片i，我们得出n个对象的候选编辑框bij,j∈n。我们对每个候选编辑框使用公式2来计算得分p(公式）。图像描述中出现的单词ω对应的得分p（公式）对应于所有从noist-OR函数计算得到的j的p（公式）的聚合。我们对所有词汇表V中的单词都这么做。请注意所有的词汇探测器早就独立的训练过，因此他们的输出需要校正。为了校正从不同探测器的输出，我们使用图像级可能性p(公式）来计算训练数据的一个扩展子集的精度。我们以全局阙值来评估这个阙值精度。然后输出所有单词V的精度或者和图像级可能性p（公式）结合的更高的精度。<br>公式2向我们展示了一些简单的MIL检测。对于每张图片，我们画出候选边界框。注意，这个方法没有使用任何的边框注释进行训练，但是仍然可以有效的将对象区域化，也可以有效将图像区域和更抽象的概念结合起来。<br>4.产生语言<br>我们将生成过程转换成搜索基于视觉检测到的单词集所能组成的最可能的句子。这个语言模型是整个过程的核心，因为它定义了单词序列的概率分布。注意虽然LM是个概率模型，但是他可以将有意义的信息进行编码，例如“跑步”更可能跟在“马”后面而不是跟在“说话”后面。这些信息可以帮助识别错误的单词检测，并将世界上的常识性知识进行编码。<br>4.1统计模型<br>为了生成图像的候选标题，我们使用基于视觉检测到的单词集的最大熵LM。最大熵LM以前面ω1，ω2……，ωl-1以及前面句子中还没提到的存在于可能性最大的检测单词表Vl中的单词,来估算ωl的概率。以没有使用的单词为条件的动机是鼓励使用所有的单词来避免重复。前十五个最频繁的封闭性词类从集合V中剔除，因为他们几乎在每幅图片中都会被检测到（而且由LM可以任意生成）。注意检测到的单词通常比较嘈杂。因此，当预测句子结束标记时，剩下的单词集可能仍然包含一些检测置信度较高的单词。<br>根据最大熵LM的定义，基于在前面出现过的单词以及后面出现的对象所推测出来的最可能的词可以写作：（公式）<br>这里的“<s>”表示句子的开始标记，fk和λk分别表示第k个最大熵及这个最大熵的权重。表1总结了我们使用的基本的离散最大熵特征。这些特征来源于我们“baseline”系统。事实证明，使用“得分”特征根据相应的视觉探测器来计算单词的对数似然是有效的。我们也用distant bigram特征和连续空间对数双线性特征做了实验，尽管这些明显改进PPLX，但是他们并没有改善BLEU,METEOR或者human preference，而且空间限制也妨碍了进一步的讨论。<br>为了训练最大熵LM，目标函数基于以相应的的检测对象集中为条件的标题的对数似然（公式）这里的上标(s)表示训练数据集中的句子索引，#(s)表示句子的长度。噪声对比评估(NCE)的技术被用于避免计算式子（3）的分母来加速训练。在生成过程中，我们使用非标准化的NCE似然估计，它比标准的NCE似然估计要有效得多，并且能产生非常相似的输出。然而，我们报告中的所有PPLX数据都是经过彻底的标准化计算的。最大熵特征应用在散列表中就像文献[29]中一样。在我们的实验中，我们使用N-gram从1个单词到4个单词的情况，以及使用了15个对比样本进行了NCE训练。<br>4.2产生过程<br>在产生阶段，我们演示了一个类似于文献[38]中使用的一种从左到右的定向搜索方法。这保持了一堆长度为l的部分假设。在研究的每一步中，每一段在堆栈上长度为l的路径都被加上一段可能的单词，得到的结果为长度l+1的路径被存储起来。前k个长度为l+1的路径被保留下来，剩下的被丢掉。<br>我们定义可能作为延长部分的是“句子结束部分”记作”</s>“，100个最频繁的单词被归类到可能被提及到的单词，所有在训练集中出现的单词都被发现跟在假设中的最后一个单词后面。剪枝基于部分路径的可能性。当“”产生时，“的完整路径将从堆栈中删除，并将保留为一个完整的句子。这个过程将一直持续到句子长度达到最大长度L。<br>在得到一组完整的句子C后，我们得到了一个如下的M-best列表。给定目标数量为T的被提及的图像属性，C中的序列包含至少T个对象被加到M-best列表中，按对数似然降序排序。如果发现C中有少于M个序列包含至少T个对象，我们将T依次减1直到M个序列被找到。<br>5.句子重排序<br>我们的语言模型生成一套M-best语句。我们最后的一个阶段使用MERT来对句子重新排序。MERT使用在整套句子的基础上计算一个线性组合特征，这些特征比如像句子长度，不能在原始的生成过程中使用。MART模型使用BLEU度量在M-best列表上训练验证集，并且应用测试集的M-best列表。最后，经过重排序的最优的序列被选中作为图片的标注。<br>随着几个标准的特征的使用，我们介绍了一个新的多模态相似模型，将在下面进行讨论。MERT所使用的特征的完整清单见表2.<br>5.1深度多模态相似性模型<br>为了表示图片和文本的全局相似性，我们提出了一个深度多模态相似性模型（DMSM）。DMSM学习到将图像和文本片段映射到公共向量中去的两个神经网络。我们用测量图像和文本相对应的向量的数量积相似性来度量图像和文本的相似性。这个数量积被MERT用来重排序句子。DMSM与基于深度网络的语义模型（DSSM）密切相关，只是将其扩展到多模态设置。DSSM最初被提出是为了对文本搜索查询和文档的语义关联性进行建模，在本工作中进行了扩展，将原始的DSSM查询向量替换为深度卷积神经网络计算的图像向量。<br>DMSM由一对神经网络组成，一个神经网络用于将每个输入进行联合训练后映射到一个共同的语义空间。在训练中，数据由一组图像、标题对组成。训练过程中的最小化的损失函数表示给定相应图像的标题的负对数后验概率。<br>图像模型：我们通过由多个卷积层、最大池化层、相应归一化和全连通层组成的深度卷积神经网络将图像映射到语义向量。这个架构在大规模的图像分类中取得了非常成功的效果，并且学习到的特征被证明可以应用在各种各样的视觉任务中。收到这些成果的激励，为了简化训练，我们没有重新学习我们模型中全部的权重。相反的，我们使用在ILSVRC2012图像分类数据集上训练好的网络初始化了一些图层，并且只在上面训练了前几个全连接层的权重。因此，虽然这个模型非常深（有12层），我们没有在前七层中训练权重值。我们根据验证集的性能交叉验证了其它层的数量。模型的全连接层最后一层的表征是给定图片的语义向量，必须与文本模型的最后一层有相同的大小，才能计算出他们之间的余弦相似度。<br>文本模型：DMSM的文本部分以与原始的DSSM相同的方式将文本段映射到语义向量上。一般来说，当文本段被视为一个可以被表示为使用固定大小的单词计数向量的词袋时，可以是一个完整的标题。根据文献[16]我们将词汇计数向量转变为letter-trigram count vector，使用将上下文相关的字母计数分布来表示一个单词。这种表示的优点是减少了输入层的大小，同时很好的归纳了不常见、没见过的以及拼写错误的单词。这种表示前向传播到一个深度全连接网络，在最后一层生成一个语义向量。<br>对象和训练：我们定义相关性R为图片或者查询和基于他们的表示形式yQ和yD的文字段或者文档Q的余弦相似性得到图像和文本模型：<br>对于给定的文本图像对，我们可以通过公式(6)来计算文本和图像相关的后验概率，这里的γ是使用验证集确定的平滑因子。D表示应该与查询（图像）相比较的一组候选文档（标题）的集合。我们发现将D限制为一个匹配的文件D+并随机的选择固定数字N的不匹配文件D-工作良好，虽然使用噪声控制估计可以进一步的改善结果。因此，对于每一张图像，我们选择一个相关的文字段和N个不相关文字段来计算后验概率。在训练过程中，我们调整模型的参数Λ，使相关字标注和匹配的图像之间的负对数后验概率最小化。（公式7）附加的细节在附录中给出。<br>6.实验结果<br>接下来我们将描述用于测试的数据集，然后评价我们的单次检测的方法和句子生成的实验结果。<br>6.1数据集<br>我们大部分的实验结果就是在MicrosoftCOCO数据集上面的汇报的。这个数据集包含82,783训练图片和40,504张校验图片。由于大部分的照片包含多个对象和中腰的上下文信息，因此这些图片为图像标注创建了一个具有挑战性的测试平台。COCO数据集为每张图片提供了五组人工注释的标题，总共有超过400k的标题。测试的注释是很难得到的，所以我们将验证集分为验证集和测试集。<br>为了与之前的论文进行试验对比，我们还报告了PASCAL语句数据及的实验结果，该数据集包含了来自2008年VOC Challenge的1000张图片，每张图片有6个人工标注的标题。<br>6.2单词检测<br>为了深入理解我们使用MIL进行单次检测的弱监督的方法，我们度量了他=这种方法在单词分类任务上的精确度。注意这是一项极具挑战性的任务，因为概念上相似的单词是单独分类的，比如说单词cat/cats/kitten,或者run/ran/running都对应不同的分类。注意，增加进一步的监督等等，在词元的前提下，无法产生显著的效果。如果一个词至少一次被用在ground truth标注中，他就数与一个图像的正类。<br>不同词性的平均查准率和人为召回的精确度结果如表3。我们发现人们经常在使用哪些单词而存在分歧，而人类因为不同的单词（通过将一个标题中的单词与其他标题对比来衡量）而存在不同的意见（见附录）。当我们考虑更多的标题时，人类的准确率会提高，但是人类的召回率保持相对不变。因此我们测量了提出的算法在人类召回率方面的精度，并且比较了人类的准确率，衡量得到人类和机器的分类能力的相似性。<br>我们报告了两个蓟县。第一个（chance)是对每个单词随机分类的结果。第二个（Classification）是用我们的MIL方法对相同的特征（CNN的fc6层）做一个简单的窗口训练所有的图像分类器得到的结果。<br>如图3显示，MIL NOR方法提高了所有词性的两个基线，证明更好地定位可以帮助预测单词。实际上，我们发现名词和形容词的提高最大，他们通常对应于图像区域中的具体对象。分类和MIL NOR的在词性方面低的结果可能因为视觉信息少而且难以检测，比如说形容词（few，有着1.94的AP），代词（比如：himself，有着1.71的 AP），和介词（比如：before，有0.68的AP）相比之下，拥有较高AP值的单词，要么是视觉信息丰富的（red：AP:62.4，her：AP：34.7）要么与特定对象相关联（polar:AP:78.6,stuffed:AP:60.3）。图2和图3展示了单词定位的定性结果。<br>6.3标题产生<br>我们接下来描述我们的标题生成结果，首先要讨论评估指标。<br>Metrics:句子生成过程使用自动指标和人工研究来度量。我们使用三个不同的自动指标：PPLX,BLEU和METEOR。PPLX测量句子模型的不确定性，对应的是给定语言模型的每个单词的编码需要多少比特。因此PPLX越低表示得分越高。BLEU广泛应用于机器翻译中，用于测量假设和参考或参考集之间的共有的N-gram（从1到4）。METEOR测量一元模型的精度和查全率，扩展精确的单词匹配包括基于WordNet同义词和词根的相似词汇。<br>所有这些自动指标都已知的只大概的和人类的判断有关。因此，我们也做了众包实验进一步的探索我们模型的质量。每个任务都给众包工作者提供一张图片和两组标题。一个是自动生成的标题，另一个是人工标题。工作人员被要求选择图片描述更好的标题，或者选择同等质量的“相同”的标题。在每组实验中，250个Turker被要求对比20组标题对，5个Turkers判断每个标题对。我们使用Crowdflower，它会自动过滤垃圾邮件的发送者。标题的顺序为了避免偏见使用了随机化，我们加入了四个答案显而易见的检查案例，遗漏这些内容的注解者被排除在外。最终的裁判结果大都出自5个Turkers的裁判。在平局的情况下，一半的计数被分布在两个最佳答案。<br>生成结果：表4总结了我们在MC COCO数据集上的标题生成结果。为了进行试验对比，我们提供了几个基线。这包含两个测量数据集复杂性的基线：Unconditioned Generation，在不知道视觉单词检测器的情况下通过对N-gram语言模型生成的句子采样；Shuffled Human，随机选择另一个人从另一幅画中生成的标题。对于这些方法，BLEU和METEOR的评分都非常低，证明MC COCO数据集有很大的多样性和复杂度。<br>我们提供了三个我们的变形算法的结果，Baseline, Baseline+Score, 和 Baseline+Score+DMSM。Baseline使用表1中所有离散的特征训练的ME LMBaseline+Score将单词检测器来给特征评分加入到ME LM。这两种方法都使用了同一组句子特征（包括DMSM评分），这组特征在第5节使用MERT重排标题顺序时描述过。Baseline+Score+DMSM使用了和Baseline+Score同样的ME LM，但是加入了DMSM得分作为特征重排序。如图4表示。带不带单词检测器得分特征的MELX的PPLX大致相同。在ME LM中增加单词检测测器分数的BLEU和METEOR都有改善；并且加入了重排序的DMSM得分进一步提高。令人惊讶的是，BLEU的分数实际上高于人工标注的标题。（21.05%和19.32%）<br>为了更好的理解我们标题的感知质量，我们报告了人们判断自动标题与人工标题的“相同质量”“更好”“相同或更好”的百分比，并从二项分布标准错误中推导出误差线。我们发现，Baseline+Score+DMSM方法生成的标题有23.3%的几率被判断为与人工标题具有相同或者更好的质量，这是对基线结果的显著改善。注意，给定的错误线假设样本是独立的：如果我们使用McNemar配对测试，即在同一组图片上比较三个系统的结果，那么我们会发现添加了DMSM的我们的系统可以使大多数人的投票情况明显改善，P的值为0.024。最终结果如图4所示。<br>为了能够直接的与以前的工作做对比，我们还在PASCAL语句数据集上做了测试，这个数据集在Midge和Baby Talk系统上都有使用。我们发现在Midge系统上的BLEU和METEOR指标都有显著的提高。Baby Talk会产生多个句子的长标题，很难用BLEU和METEOR做比较。然而，为了给定一个对这个领域飞速发展的基本的定性认识，图4展示了我们系统，Midge系统和Baby Talk系统的输出。</p>
</section><nav id="post-nav"><span class="prev"></span><span class="next"><a href="/2019/02/23/2019-to-do-list/">Older Posts<span class="arrow">→</span></a></span></nav></article></section><footer id="footer"><div id="social"><a class="symbol" href="https://github.com/jiyali"><i class="fa fa-github"></i></a></div><p class="small">© Copyright 2018 &nbsp;<i class="fa fa-heart" aria-hidden="true">&nbsp;JYL</i></p><p class="small">Powered by &nbsp;<a href="https://hexo.io/">Hexo &nbsp;</a></p></footer><script>hljs.initHighlightingOnLoad();</script></body></html>