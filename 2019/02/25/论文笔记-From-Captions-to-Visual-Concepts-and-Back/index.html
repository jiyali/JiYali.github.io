<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1"><title>论文笔记-From Captions to Visual Concepts and Back</title><link rel="shortcut icon" href="/images/avatar.png"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.2.0/styles/github.min.css"><script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script></head><body><nav class="main-nav"><a href="/">Home</a><a href="/archives">Archives</a></nav><div class="profile"><section id="wrapper"><header id="header"><img class="2x" id="avatar" src="/images/avatar.png"><h1>Coding With Yali</h1><h2></h2></header></section></div><section class="post" id="wrapper"><article><header><h1>论文笔记-From Captions to Visual Concepts and Back</h1><h2 class="headline">Feb 25, 2019 2:24·3.5k words
·12 minutes read<span class="tags"></span></h2></header><section id="post-body"><p>摘要<br>本文展示了一种自动生成图像描述的新方法：直接地从图像标题数据集中学习的视觉检测器和语言模型。我们使用的是多示例学习去训练在标题中常见词的视觉检测器，这些常见词包括很多不同的词性包括名词、动词和形容词。单词检测器的输出用作最大熵语言模型的条件输入。这个语言模型从一组超过400,000图像描述中学习，来捕捉单词用法的统计量。我们通过使用句子级的特征和多模态相似性模型来对标题候选者的重新排序，从而获得全局语义。<br>1.前言<br>什么时候说明机器理解了图片呢？一个定义是：当它可以生成一个新的标题来概述图片中突出的内容比如图片中出现了什么目标、他们的特征、或者目标之间的关系时，就说机器理解了图片。决定主要内容的不仅需要知道图片内容，而且还需要通过常识推断出场景的哪个方面可能有意思或者很新颖。<br>本文描述了一个从样本中生成图像标题的新方法。我们从图片以及相应的图像描述数据集中训练我们的标题生成器。以前生成图像标题的方法依赖于目标、特征和从单独的手动标注的训练集中学习到的关系探测器。<br>在训练中直接使用标注有两个明显的优势。首先，标题只是包含内在的显著的信息。比如说一个从包含“小狗”这个单词的很多有标题的图片中训练出来的小狗探测器可能偏向于探测突出的小狗，而不是背景中的小狗。图像描述也包括各种各样的单词，包括名词、动词还有形容词。因此，我们需要学习与各种概念相关的探测器。当有些概念，比如说“骑”或者“漂亮”，在理论上可能非常难学习。这些术语可能与特定的视觉形态高度相关（如一个人骑在马上或者日落时在山上。<br>其次，在图像标题上训练语言模型可以获取关于场景中的常识性知识。一个语言模型可以学习到一个人更可能坐在椅子上而不是站在椅子上。这个信息消除了噪声视觉检测的歧义。<br>我们的方法流程如图一所示。首先我们用弱监督学习为一组图像标注中的常用单词构造一个检测器。直接从图像标注中学习是很难的，因为系统并不能利用像对象候选边界框这种在别的数据集中找到的监督信号。很多单词，比如说“拥挤”或者“内部”，甚至没有定义良好的候选边界框。为了克服这个难点，我们使用三个想法。首先，系统发现图像领域使用目标建议生成器。下一步，我们使用预处理的ImageNet卷积神经网络特征对每个区域进行特征化。最后，我们把每个区域的特征映射到可能包含在标注中的一组词汇中去。我们用多示例学习来训练这个映射去确定每个单词信息量最大的图像区域。<br>从可能的词袋产生新的图像描述需要有效的语言模型。在本文中，我们把标注生成看做一个优化问题。这样看来，核心任务就是拿到一套单词检测的分数集，然后找出每个单词都恰好包含一次的最大可能性的句子。我们从一组训练图像描述中训练出一个最大熵语言模型。这种训练通过语言统计学获得关于这个世界的常识性知识。对单词序列进行显示搜索可以有效找出一组可能性高的句子。<br>系统的最后一阶段通过对句子特征进行加权处理重排了这组高似然句子。这些权重通过最小错误率训练学习得到。除了几个普通的句子特征，我们还介绍了一个基于深度多模态相似性模型的新的特征。深度多模态相似性模型学习两个神经网络将图像和文字片段映射到一个公共向量上，这样就能很容易的测量句子和图片之间的相似性。我们论证了，深度多模态相似性模型显著提高了选择句子的质量。<br>为了鉴定我们自动标注的质量，我们用了三个易于计算的指标，以及在“Amazon’s Mechanical Turk”上与人类受试者做了更好/更差/相等的比较。测试是在具有挑战性的”Microsoft COCO“数据集上进行的，这个数据集包含复杂的多个目标的照片。82,783训练图片每一个都有人工注释的标题。为了测定我们句子的质量，我们使用了流行的”BLEU,METEOR还有perplexity (PPLX)”指标。我们惊喜的发现，我们生成的标注在“BLEU”指标上尽然胜过了人类。当人类受试者评估时，受试者发现我们的标注在23%的情况下有着与人类同等质量甚至超过人类。我们也比较了以前在“PLSCAL”语句数据集上的准确性。我们的结果表明，直接在图像标注上同时训练视觉探测器和语言模型和使用全局语义模型来重排标题候选者都是非常有效的。<br>2.相关工作<br>有两个训练良好的自动生成图像标题的方法：检索现有的人工标注的图像标题，生成新的标题。最近基于检索的方法已经使用神经网络将文本和图像映射到一个公共向量中。其他基于检索的方法使用的是预定义的图像特征的相似性指标。Farhadi等人提出将图像和文字作为一个linguistically-motivated（暂时不懂怎么翻译）语义三元组，然后在这个语义三元组中计算相似性。在神经网络的环境下对句子和图像进行了相似的细粒度分析发现对检索的用处不大。<br>基于检索的方法经常返回一个结构良好的人工标注的标题，但是这些标题可能不能很好的描述新目标的组合或者一个新的句子。这种限制导致了大量的工作，首先需要分析图像和图像中检测到的对象，然后生成新的标题。我们把之前在生成过程中使用的语法和语义的限制的研究和当前最先进的技术进行了比较。我们研究了利用最大似然估计结合句法结构生成新句子的”Midge“系统，并将它与选择通过条件随机场预测的最可能的图像标签来填充句子模板的”Baby Talk“系统进行了定性比较。这两个先前的系统使用了相同的测试句子集，使得直接对比两个系统成为了可能。<br>最近，研究人员探索了从图像中引导语言模型的纯粹的统计方法。Kiros等人从图像派生出的文本模型中提取出图像中的偏置特征使用“log-bilinear”模型，Kiros的模型通过采样对标题进行排序和生成。Mao等人探索了图像条件递归神经网络的使用。这两种方法都没有使用明确的出事对象检测步骤。<br>类似于最后两种方法，我们的研究重点为以图像内容为引导的语言生成。不同于这些方法的是，我们通过将CNN应用到图像区域中检测图像中的单词，并将这些单词用MIL进行信息整合。我们还通过直接从标题训练来最小化句子结构的先验假设。最后，与Kiris,Mao相比，我们将生成问题当做一个最优化问题，并寻找最可能的句子。<br>3.字检测<br>我们生成标题流程的第一步是检测出一组最可能是图像描述的一部分的单词。这些单词可能属于任意一种词性，包括动词，名词和形容词。我们规定我们的词汇表V使用1000个训练标题中的常用词汇。这个词汇表包含了训练标题中出现的92%的单词。整个词汇表会在附录中展示出来。<br>3.1训练单词检测器<br>给出了词汇表，我们下一步的目标是从图像中检测单词。当我们不知道与单词对应的图像候选边界时我们不能使用标准监督学习技术。事实上，很多单词比如像“打开”或者“漂亮”的含义并不能很容易的定义它的候选边界框。一个可能的方式是使用图像分类器作为整个图像的输入。在我们展示的第6节里，这将导致性能下降因为很多单词或概念只适用于图像的子区域。相反的，我们用多实例学习的弱监督方法来训练我们的探测器。<br>属于词汇表V的每个单词ω，MIL将候选边界框的“正”“负”袋作为输入集。每个袋相当于一张图像。对于单词ω和图像i，如果单词ω在图像i的描述中时，一个包bi就会将其描述为“正”,反之为负。直观地来说，MIL通过迭代的在“正”包中寻找“正”实例来进行训练，接着用更新“正”标签的方法反复训练探测器。使用Zitnick and Doll´ar的对象建议生成器Edge Boxes 70可以生成每张照片64组候选边界框的计算效率。<br>我们使用noisy-OR版本的MIL，这个版本中包含单词ω的包bi的概率是由词袋中单个实例的概率计算的：<br>这里的（公式）包含单词ω包bi的概率取决于包中每个实例的概率：<br>这里的（公式）是图像i对应的单词ω的候选边界框j的概率。对于每个候选边界框，我们将ImageNet数据放到卷积神经网络的fc6层去训练来计算特征，这些特征已经被证明不仅仅适用于ImageNet的分类任务，而且还适用于各种视觉任务。我们对图像i中的选择框j的fc6的特征Φ(bij)使用了逻辑函数（公式）来计算p(公式）。这里的vω与单词有关系的权重，uω是偏差<br>3.2对测试图片生成单词<br>给定一个新的测试图片i，我们得出n个对象的候选编辑框bij,j∈n。我们对每个候选编辑框使用公式2来计算得分p(公式）。图像描述中出现的单词ω对应的得分p（公式）对应于所有从noist-OR函数计算得到的j的p（公式）的聚合。我们对所有词汇表V中的单词都这么做。请注意所有的词汇探测器早就独立的训练过，因此他们的输出需要校正。为了校正从不同探测器的输出，我们使用图像级可能性p(公式）来计算训练数据的一个扩展子集的精度。我们以全局阙值来评估这个阙值精度。然后输出所有单词V的精度或者和图像级可能性p（公式）结合的更高的精度。<br>公式2向我们展示了一些简单的MIL检测。对于每张图片，我们画出候选边界框。注意，这个方法没有使用任何的边框注释进行训练，但是仍然可以有效的将对象区域化，也可以有效将图像区域和更抽象的概念结合起来。<br>4.产生语言<br>我们将生成过程转换成搜索基于视觉检测到的单词集所能组成的最可能的句子。这个语言模型是整个过程的核心，因为它定义了单词序列的概率分布。注意虽然LM是个概率模型，但是他可以将有意义的信息进行编码，例如“跑步”更可能跟在“马”后面而不是跟在“说话”后面。这些信息可以帮助识别错误的单词检测，并将世界上的常识性知识进行编码。<br>4.1统计模型<br>为了生成图像的候选标题，我们使用基于视觉检测到的单词集的最大熵LM。最大熵LM以前面ω1，ω2……，ωl-1以及前面句子中还没提到的存在于可能性最大的检测单词表Vl中的单词,来估算ωl的概率。以没有使用的单词为条件的动机是鼓励使用所有的单词来避免重复。前十五个最频繁的封闭性词类从集合V中剔除，因为他们几乎在每幅图片中都会被检测到（而且由LM可以任意生成）。注意检测到的单词通常比较嘈杂。因此，当预测句子结束标记时，剩下的单词集可能仍然包含一些检测置信度较高的单词</p>
</section><nav id="post-nav"><span class="prev"></span><span class="next"><a href="/2019/02/23/2019-to-do-list/">Older Posts<span class="arrow">→</span></a></span></nav></article></section><footer id="footer"><div id="social"><a class="symbol" href="https://github.com/jiyali"><i class="fa fa-github"></i></a></div><p class="small">© Copyright 2018 &nbsp;<i class="fa fa-heart" aria-hidden="true">&nbsp;JYL</i></p><p class="small">Powered by &nbsp;<a href="https://hexo.io/">Hexo &nbsp;</a></p></footer><script>hljs.initHighlightingOnLoad();</script></body></html>